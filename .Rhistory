# if you need to view it
# View(aispmydata)
# if you need it outside the output file
#write.table(motable.mydata, file="./aispmydata.csv", quote = FALSE, sep = "\t",row.names = FALSE)
knitr::kable(Hvalues$Hi, caption = Tab_2.1_cap)
knitr::kable(motable.mydata, caption = Tab_2.2_cap)
Tab_2.3a_cap <- table_nums(name="tab_2.3a", caption = "MSA: Subcale 1: item homogeneity values")
Tab_2.4a_cap <- table_nums(name="tab_2.4a", caption = "MSA: Subcale 1: monotonicity with default minsize")
Tab_2.5a_cap <- table_nums(name="tab_2.5a", caption = "MSA: Subcale 1: IIO with default minsize")
Fig_2.2a_cap <- figure_nums(name="fig_2.2a", caption = "MSA: Subcale 1: ISRF with minsize=50" )
Fig_2.3a_cap <- figure_nums(name="fig_2.3a", caption = "MSA: Subcale 1: IIO with minsize=50" )
# select the most appropriate solution, for example code below is for the solution at lowerbound .30
myselection <- aisp(mydata,  lowerbound=.3)
# myselection
# check which items are in which subscales (here the first subscale)
names(mydata[,myselection==1])
# check properties for subscales:
# select the first subscale (if MSA confirms the initial 3-subscale structure)
mysubscale1 <- mydata[,Subscale1]
# check H
HvaluesSubscale1 <- coefH(mysubscale1)
knitr::kable(HvaluesSubscale1$Hi, caption = Tab_2.3a_cap)
# check conditional association (local independence)
CA.def.mysubscale1 <- check.ca(mysubscale1, TRUE)
CA.def.mysubscale1$InScale
CA.def.mysubscale1$Index
CA.def.mysubscale1$Flagged
# check monotonicity at different minsize:
# with default minsize:
monotonicity.def.mysubscale1 <- check.monotonicity(mysubscale1, minvi = .03)
knitr::kable(summary(monotonicity.def.mysubscale1), caption = Tab_2.4a_cap)
# try different minsizes 60 to 10
monotonicity.59.mysubscale1 <- check.monotonicity(mysubscale1, minvi = .03, minsize = 59)
summary(monotonicity.59.mysubscale1)
plot(monotonicity.59.mysubscale1, ask=FALSE)
monotonicity.50.mysubscale1<- check.monotonicity(mysubscale1, minvi = .03, minsize = 50)
summary(monotonicity.50.mysubscale1)
#plot(monotonicity.50.mysubscale1)
# plot ISRFs in a pdf
#pdf( "./ISRFs-mydata1.pdf", width=5*2, height=2*2, paper="special" );
par( mfrow=c(2,5));
#plot(monotonicity.50.mysubscale1, curves="ISRF", ask=FALSE, color.ci="yellow")
plot(monotonicity.59.mysubscale1, ask=FALSE, color.ci="yellow")
#dev.off();
# Investigate the assumption of non-intersecting item step response functions (ISRFs)
# using method MIIO (appropriate for ordinal items)
miio.mysubscale1 <- check.iio(mysubscale1)
# or using rest score (for binary items)
# restscore.mysubscale1 <- check.restscore(mysubscale1)
# several other options are available in mokken: pmatrix, mscpm, and IT
knitr::kable(summary(miio.mysubscale1)$item.summary, caption = Tab_2.5a_cap)
# Investigate the assumption of non-intersecting item step response functions (ISRFs) at different minsize values
miio.59.mysubscale1 <- check.iio(mysubscale1, method="MIIO", minsize = 59)
summary(miio.59.mysubscale1)
miio.50.mysubscale1 <- check.iio(mysubscale1, minsize = 50)
summary(miio.50.mysubscale1)
#miio.30.mysubscale1 <- check.iio(mysubscale1, minsize = 30)
#summary(miio.30.mysubscale1)
par( mfrow=c(6,5));
plot(miio.50.mysubscale1, ask=FALSE)
#__________________#
####   STEP 3   ####
#__________________#
# PIRT ####
# examine PIRT model fit for the unidimensional scales identified via NIRT
# for binary items - Rasch model
#fit1.Subscale1 <- RM(mydata[,Subscale1])
# for ordinal items - Rating Scale model (if all items have the same format, e.g. all are on 5-point scales from strongly agree to strongly disagree)
fit1.Subscale1 <- RSM(mydata[,Subscale1]#, se=TRUE, sum0=TRUE#, constrained = FALSE
)
# this is a section of R script that prepares the analysis; it imports and uploads some libraries which will be used later on, and defines some useful settings and functions.
# here you may want to add some more advanced functions later on
# there is no need to modify anything for a basic analysis
# install and upload the relevant libraries ####
for (n in c('foreign', 'car', 'utils', 'relimp', 'ggplot2', 'ggdendro',
'dplyr', 'tidyr', 'reshape2', 'tibble', 'captioner', 'stargazer',
'psych', 'memisc', 'Hmisc', 'ltm', 'MASS',
'lavaan','semTools','semPlot', 'qgraph','sem',
'mirt', 'eRm', 'mokken', 'rgl','scales',
'CTT','MBESS', 'knitr', 'bookdown','data.table', 'readr', 'TAM', 'plyr', 'WrightMap', 'eRm'))
{
if(!require(n,character.only=TRUE))
{
stop(paste0("Package '",n,"' is not installed: please install it and try again!\n"));
# install.packages(n)
}
library(n,character.only=TRUE)
}
#  brief info on the uses of these libraries:
#  for importing data: 'foreign'
#  for data viewing, writing: 'utils', 'relimp'
#  for plotting: 'ggplot2', 'ggdendro'
#  for data management: 'dplyr', 'tidyr', 'reshape2', 'tibble',
#  for descriptives and various psychometric analyses: 'psych', 'memisc', 'Hmisc',
#  for latent trait models, incl calpha, factor scores, IRT: 'ltm', 'lavaan','semTools','semPlot','qgraph','sem',
#  for parametric IRT analysis: 'mirt', 'eRm'
#  for Mokken scale analysis (nonparametric IRT): 'mokken'
#  for plotting:'rgl','scales',
#  for classical test theory: 'CTT','MBESS'
#  for reporting documents: 'knitr', 'bookdown'
# ^^^^^ check out other psychometrics packages at: http://cran.r-project.org/web/views/Psychometrics.html
# if you want to install them all, use the code below (installs cran task views package,
# then uses install.views to get all updated packages in Psychometrics):
# install.packages("ctv")
# library(ctv)
# install.views("Psychometrics")
# add captioning for figures and tables (as here: https://www.r-bloggers.com/r-markdown-how-to-number-and-reference-tables/)
table_nums <- captioner::captioner(prefix = "Table")
figure_nums <- captioner::captioner(prefix = "Figure")
t.ref <- function(x) {
stringr::str_extract(table_nums(x), "[^:]*")
}
f.ref <- function(x) {
stringr::str_extract(figure_nums(x), "[^:]*")
}
# add functions needed in analyses
# defines a function to partition an item set into mokken scales - lowerbound from .05 to .80
moscales.for.lowerbounds <- function( x, lowerbounds=seq(from=0.05,to=0.80,by=0.05) )
{
ret.value <- NULL;
for( lowerbound in lowerbounds )
{
tmp <- aisp( x,  lowerbound=lowerbound );
if( is.null(ret.value) )
{
ret.value <- data.frame( "Item"=rownames(tmp), "Scales."=tmp[,1] );
}
else
{
ret.value <- cbind( ret.value, "Scales."=tmp[,1] );
}
names(ret.value)[ncol(ret.value)] <- sprintf("%.2f",lowerbound);
}
rownames(ret.value) <- NULL;
ret.value;
}
##Made this to write to csv the data I need for the scale validation
#stnicsv<-data.table(dp$dataQL[VisitID=="BL"]$stni1, dp$dataQL[VisitID=="BL"]$stni2, dp$dataQL[VisitID=="BL"]$stni3, #dp$dataQL[VisitID=="BL"]$stni4, dp$dataQL[VisitID=="BL"]$stni5, dp$dataQL[VisitID=="BL"]$stni6, dp$dataQL[VisitID=="BL"]$stni7, #dp$dataQL[VisitID=="BL"]$stni8, dp$dataQL[VisitID=="BL"]$stni9, dp$dataQL[VisitID=="BL"]$stni10a, dp$dataQL[VisitID=="BL"]$stni11a)
#colnames(stnicsv)<-c("stni1", "stni2", "stni3", "stni4", "stni5", "stni6", "stni7", "stni8", "stni9", "stni10a", "stni11a")
#write.csv(na.omit(stnicsv))
# this is a section of R script for importing data from other formats and checking it to make sure you imported the right one
# you can modify depending on the format and name of your dataset
# Here are examples of how you can import data from csv, excel and SPSS
### from csv:
mydata <- na.omit(read.table(file="R:\\MIRECC\\STUDIES\\Hendrickson_1588176-01881_ATTEND\\Study Data\\Aaron's Folder\\stni89BLrandoraw1.csv",
header=TRUE, # first row contains variable names
sep=","))   # comma is separator
#                      na.strings="999", # if you have specific numbers for missing data, if not, comment this line out
#                      row.names="id") # assign the variable id to row names
## OR use file.choose() to select your dataset manually via a window interface
### from excel:
# library(xlsx)
# mydata <- read.xlsx("c:/myexcel.xlsx", 1) # read in the first worksheet from the workbook myexcel.xlsx, first row contains variable names (default)
# OR
# mydata <- read.xlsx("c:/myexcel.xlsx", sheetName = "mysheet") # read in the worksheet named mysheet
### from SPSS:
# ***** first save SPSS dataset in portable format
# SPSS syntax:
# get file='c:\mydata.sav'.
# export outfile='c:\mydata.por'.
# then in R, import your file (function in Hmisc package)
# mydata <- spss.get("c:/mydata.por", use.value.labels=TRUE) # last option converts value labels to R factors
### In the psych package, there is also an option to copy paste the data from clipboard
# mydata <- read.clipboard()
#### more on importing data from sas, stata or systat: http://www.statmethods.net/input/importingdata.html
### import data files with an .RData format (e.g. exported at the end of data preparation in a prior R script) ####
#for( s in c("preparedDataSet",
#   "otherpreparedDataSet" ))
#{
# load(paste0(s,".RData"));
#}
# give the name 'mydata' to the dataset you want to work with (this name will be used for all analyses below)
# for example, if the item set you want to study is the first 30 variables of the preparedDataSet, the code would be:
#mydata <- preparedDataSet[1:30]
# check data
# View(mydata)
# OR
# headTail(mydata)
# check sample size
# nrow(mydata)
# check variable names
# names(mydata)
# if needed, change variable names:
#names(mydata) <- c("Item1","Item2","Item3","Item4","Item5","Item6","Item7","Item8","Item9","Item10","Item11","Item12",
# "Item13","Item14","Item15","Item16","Item17","Item18","Item19","Item20","Item21","Item22","Item23",
# "Item24","Item25","Item26","Item27","Item28","Item29","Item30")
# use variable names that are meaningful for you (instead of "Item1",...) and make sure you rename all variables and follow the column order!
# to change a single variable name, use:
# names(mydata)[names(mydata) == "Item7"] <- "mymeaningfulnamehere"
# define subscales - if you already have a hypothesis about which items belong to which subscales
# for example, if your 30-item scale includes 3 subscales with 10 items each recorded in this order in the dataset, the code would be:
#THIS IS DONE TO TRY AND CONVERT MYDATA TO A DICHOTOMOUS SCALE, 3 and 4 ARE CONVERTED TO 1, 1 AND 2 ARE CONVERTED TO 0
#mydata[mydata==1]<-0
#mydata[mydata==2]<-1
#mydata[mydata==3]<-1
#mydata[mydata==4]<-1
Subscale1 <- names(mydata)
#Subscale2 <- names(mydata)[11:20]
#Subscale3 <- names(mydata)[21:30]
# define a vector with all your items for easy selection later on
myitems <- names(mydata)
# run frequencies for all items
for( n in myitems)
{
cat( "\n", n, ":" );
print( table( mydata[,n], exclude=NULL ) );
}
# check if the variables are coded as numeric
class(mydata[,1])
# compute new variables ordinal numeric & 'missing' as NA
## if required, recode the missing label (e.g. "Missing value") as NA
#for( n in myitems)
#{
#   mydata[ mydata[,n] == "Missing value",  n ] <- NA;
# }
# for example, define as numeric from 0 to 6 some items that were labeled as character, from 1 to 7:
#for( n in myitems)
#{
#   mydata[,n] <- as.numeric(mydata[,n])-1
# }
# or if your items are yes/no variables with missings and "don't know" answers:
# for( n in myitems)
# {
#   n1 <- paste(n,"YN",sep=""); # this adds a YN at the end of the new variable name, to avoid overwriting the dataset
#   mydata[ , n1 ] <- NA;
#   mydata[ mydata[,n] == "yes", n1 ] <- 1;
#   mydata[ mydata[,n] == "no",  n1 ] <- 0;
#   mydata[ mydata[,n] == "I don't know",  n1 ] <- 0;
#  mydata[ mydata[,n] == "Missing value",  n1 ] <- 0;
#   cat( "\n", n1, ":" );
#   print( table( mydata[,n1], exclude=NULL ) );
# }
# myitems <- names(mydata)[30:60]
# mydata <- mydata[,myitems]
# check if variables look the same after modifications
for( n in myitems)
{
cat( "\n", n, ":" );
print( table( mydata[,n], exclude=NULL ) );
}
# check if the transformation to numeric worked
class(mydata[,1])
# if required, use this code to select only cases without missing values on specific columns
# names(mydata)
# mydata.full <- mydata[complete.cases(mydata[1:10]),]
#_______________________#
####   THE 6 STEPS   ####
#_______________________#
#__________________#
####   STEP 1   ####
#__________________#
# define captions
Tab_1.1_cap <- table_nums(name="tab_1.1", caption = "Frequencies of item response options" )
Tab_1.2_cap <- table_nums(name="tab_1.2", caption = "Descriptive statistics of all items" )
Fig_1.1_cap <- figure_nums(name="fig_1.1", caption = "Barplots of high score frequencies" )
Fig_1.2_cap <- figure_nums(name="fig_1.2", caption = "Barplots of item score distributions" )
Fig_1.3_cap <- figure_nums(name="fig_1.3", caption = "Heatplot Spearman correlations between item scores" )
Fig_1.4_cap <- figure_nums(name="fig_1.4", caption = "Multivariate outliers in item set" )
# frequencies table - here example for ordinal items with 7-point Likert response scales
# build a table with item distributions
# 1. build an empty txt file
summaries.file <- "./summaries.txt";
cat( "Variable\tNegative\t1\t2\tNeural\t4\t5\tPositive\tNegativeto3\t4toPositive\tmissing\n", file=summaries.file, append=FALSE );
# 2. write a function to include values for an item
write.summary.var <- function( x, xname )
{
a1 <- sum( x == 0, na.rm=TRUE );
a2 <- sum( x == 1, na.rm=TRUE );
a3 <- sum( x == 2, na.rm=TRUE );
a4 <- sum( x == 3, na.rm=TRUE );
#  a5 <- sum( x == 4, na.rm=TRUE );
#  a6 <- sum( x == 5, na.rm=TRUE );
# a7 <- sum( x == 6, na.rm=TRUE );
a8 <- round((sum( x <=2, na.rm=TRUE )*100/nrow(mydata)), 2);
a9 <- round((sum( x >2, na.rm=TRUE )*100/nrow(mydata)), 2);
a10 <-sum(is.na(x));
cat( paste( xname, "\t", a1, "\t", a2, "\t", a3, "\t", a4, "\t", #a5, "\t", #a6, "\t", a7, "\t",
a8, "\t", a9, "\t", a10, "\n", sep="" ), file=summaries.file, append=TRUE );
}
# 3. for each item, run this function iteratively
for( n in myitems)
{
write.summary.var( mydata[,n], n );
}
# and read the table in R
myitemssum = read.table( file="./summaries.txt", header = TRUE, sep = "\t", quote="\"", fill=TRUE )
# 4. add column names
colnames(myitemssum) <- c("Item label","0", "1","2","3",
#"5", "Positive",
"% 0 to 2", "% 3 to 4", "No. missing")
# 5. view
# View(myitemssum)
# if you want to add item content, modify the quotes below with specific item content
# Item <- c("Item1","Item2","Item3","Item4","Item5","Item6","Item7","Item8","Item9","Item10","Item11","Item12",
#           "Item13","Item14","Item15","Item16","Item17","Item18","Item19","Item20","Item21","Item22","Item23",
#           "Item24","Item25","Item26","Item27","Item28","Item29","Item30")
# myitemssum <- cbind(Item, myitemssum)
# # view
# View(myitemssum)
# order items based on percentage
myitemssumOrder <- myitemssum[order(myitemssum[,9]),] # based on the 10th column (% high scores)
# Note: myitemssum[order(-myitemssum[,10]),] would do it in descending order
# view
View(myitemssumOrder)
# if you need it outside the output file
write.table(myitemssumOrder, file="./myitemssumOrder.csv", quote = FALSE, sep = "\t",row.names = FALSE)
# descriptives ordinal items
# describe treats all variables as numeric, so use it only for ordinal to ratio when properly coded
descrmyitems <- as.data.frame( round( psych::describe( mydata ), 2 ))
# View(descrmyitems)
# if you need it outside the output file
write.table(descrmyitems, file="./descrmyitems.csv", quote = FALSE, sep = "\t",row.names = FALSE)
knitr::kable(myitemssumOrder, caption = Tab_1.1_cap)
knitr::kable(descrmyitems[,c(3,4,8,9,11,12,13)] , caption = Tab_1.2_cap)
# plots to visualize the data ####
# barplot of endorsement frequencies (number of respondents with high scores)
barplot(myitemssumOrder[,"% 3 to 4"],
main = "High score frequencies for items",
xlab="Items",
ylab="Number of respondents",
cex.lab=0.8,
cex.axis=0.8,
names.arg=myitemssumOrder[, "Item label"],
las=2,
cex.names=0.6)
# barplots for items - here example for ordinal 7-point Likert
# if you want to print as pdf, use the code below and modify as needed
# pdf( "./myitemplots.pdf", width=6*2, height=5*2, paper="special" );
#jpeg( "./myitemplots.jpg", width=6*2, height=5*2, units="in", quality = 90, res=150 );
par( mfrow=c(3,4)); # set several plots per rows and columns
for( n in myitems)
{
distr <- table(mydata[,n])
barplot(distr,
main=n,
col=gray.colors(20),
ylab = "Number of respondents",
xlab = "Response");
};
# dev.off();
# prepare corelation matrix
# for binary items, use tetrachoric correlation matrix
# bluesqs <- as.data.frame(tetrachoric(mydata)["rho"]);
# for ordinal items use Spearman correlations
bluesqs <- cor(mydata, method = "spearman");
# heat plot of correlations matrix
# uncomment the png & devoff lines if you want to save as png in the working directory
# png('corplot.png')
cor.plot(bluesqs, numbers=TRUE, main="Correlations Between Items", upper=FALSE,
cex=1.5, cex.axis=1.5)
gr <- colorRampPalette(c( "white", "blue"))
iicor<-cor.plot(bluesqs, numbers=TRUE, colors=TRUE, show.legend =1, main="Correlations Coefficients Between STNI Items", upper=FALSE,
cex=1.5, cex.axis=1.3, gr=gr, zlim = c(0,1), labs=c("1", "2","3","4","5","6","7","8","9","10","11"), xlas=2)
# dev.off()
# if required, recode items that were worded in the opposite way
# mydata[,1] <- 8-mydata[,1] # where 1 is the number of the column with the variable that needs recoding
# # repeat the plot and comment the previous one if you want to display this one
# cor.plot(cor(mydata, method = "spearman"),numbers=TRUE,main="correlations between items",
#          cex=0.5, cex.axis=0.7)
# if required, exclude from next analyses items that have no/little variance
# (e.g. <5% in one category for binary variables, less than 5% in 2 adjacent response options for ordinal variables)
# mydata <- subset( mydata, select = -namevar)
# check what you have left
# names(mydata)
# check outliers in item sets
d2mydata <- outlier(mydata, cex=.6, bad=3, ylim=c(0,130))
# hist(d2mydata)
# possible to exclude by case number, for example case 1 (a first test participant shows up as outlier)
# mydata <- mydata[-1,]
# outliers are <.001 according to Tabachnick, B.G., & Fidell, L.S. (2007). Using Multivariate Statistics (5th Ed.). Boston: Pearson. (p. 74)
# explained here for SPSS: http://www-01.ibm.com/support/docview.wss?uid=swg21480128
#__________________#
####   STEP 2   ####
#__________________#
# the mokken and eRm packages - IRT analyses
# ^^^^^^ before you start, check these as guide docs:
# Non-parametric IRT: https://www.jstatsoft.org/article/view/v020i11
# Parametric IRT: https://www.jstatsoft.org/article/view/v020i09
# NIRT ####
# examine item set structure with minimum assumptions
# define captions
Tab_2.1_cap <- table_nums(name="tab_2.1", caption = "MSA: Homogeneity values (and standard errors) for items" )
Tab_2.2_cap <- table_nums(name="tab_2.2", caption = "MSA: *aisp* for increasing H thresholds (c)")
Fig_2.1_cap <- table_nums(name="fig_2.1", caption = "MSA: Guttman errors for all item set" )
# Outliers
xPlus   <- rowSums(mydata)
gPlus   <- check.errors(mydata)$Gplus
hist(gPlus)
oPlus   <- check.errors(mydata, TRUE, TRUE)$Oplus
# cor(cbind(oPlus, gPlus, xPlus))
Q3 <- summary(gPlus)[[5]]
IQR <- Q3 - summary(gPlus)[[2]]
outlier <- gPlus > Q3 + 1.5 * IQR
# if needed to further analyse ouliers
# cbind(mydata, gPlus)[outlier,]
# then possible sensitivity analysis:
# coefH(mydata[!outlier,])
# Compute scalability coefficients
Hvalues <- coefH(mydata)
# examine aisp for increasing c levels (run the function you defined above and give it a name)
motable.mydata <- moscales.for.lowerbounds( mydata,lowerbounds=seq(from=0.05,to=0.65,by=0.05))
#motable.mydata <- moscales.for.lowerbounds( mydata, lowerbounds=seq(from=0.05,to=0.80,by=0.05))
# save it as a data frame
aispmydata <- as.data.frame(motable.mydata)
# if you need to view it
# View(aispmydata)
# if you need it outside the output file
#write.table(motable.mydata, file="./aispmydata.csv", quote = FALSE, sep = "\t",row.names = FALSE)
knitr::kable(Hvalues$Hi, caption = Tab_2.1_cap)
knitr::kable(motable.mydata, caption = Tab_2.2_cap)
Tab_2.3a_cap <- table_nums(name="tab_2.3a", caption = "MSA: Subcale 1: item homogeneity values")
Tab_2.4a_cap <- table_nums(name="tab_2.4a", caption = "MSA: Subcale 1: monotonicity with default minsize")
Tab_2.5a_cap <- table_nums(name="tab_2.5a", caption = "MSA: Subcale 1: IIO with default minsize")
Fig_2.2a_cap <- figure_nums(name="fig_2.2a", caption = "MSA: Subcale 1: ISRF with minsize=50" )
Fig_2.3a_cap <- figure_nums(name="fig_2.3a", caption = "MSA: Subcale 1: IIO with minsize=50" )
# select the most appropriate solution, for example code below is for the solution at lowerbound .30
myselection <- aisp(mydata,  lowerbound=.3)
# myselection
# check which items are in which subscales (here the first subscale)
names(mydata[,myselection==1])
# check properties for subscales:
# select the first subscale (if MSA confirms the initial 3-subscale structure)
mysubscale1 <- mydata[,Subscale1]
# check H
HvaluesSubscale1 <- coefH(mysubscale1)
knitr::kable(HvaluesSubscale1$Hi, caption = Tab_2.3a_cap)
# check conditional association (local independence)
CA.def.mysubscale1 <- check.ca(mysubscale1, TRUE)
CA.def.mysubscale1$InScale
CA.def.mysubscale1$Index
CA.def.mysubscale1$Flagged
# check monotonicity at different minsize:
# with default minsize:
monotonicity.def.mysubscale1 <- check.monotonicity(mysubscale1, minvi = .03)
knitr::kable(summary(monotonicity.def.mysubscale1), caption = Tab_2.4a_cap)
# try different minsizes 60 to 10
monotonicity.59.mysubscale1 <- check.monotonicity(mysubscale1, minvi = .03, minsize = 59)
summary(monotonicity.59.mysubscale1)
plot(monotonicity.59.mysubscale1, ask=FALSE)
monotonicity.50.mysubscale1<- check.monotonicity(mysubscale1, minvi = .03, minsize = 50)
summary(monotonicity.50.mysubscale1)
#plot(monotonicity.50.mysubscale1)
# plot ISRFs in a pdf
#pdf( "./ISRFs-mydata1.pdf", width=5*2, height=2*2, paper="special" );
par( mfrow=c(2,5));
#plot(monotonicity.50.mysubscale1, curves="ISRF", ask=FALSE, color.ci="yellow")
plot(monotonicity.59.mysubscale1, ask=FALSE, color.ci="yellow")
#dev.off();
# Investigate the assumption of non-intersecting item step response functions (ISRFs)
# using method MIIO (appropriate for ordinal items)
miio.mysubscale1 <- check.iio(mysubscale1)
# or using rest score (for binary items)
# restscore.mysubscale1 <- check.restscore(mysubscale1)
# several other options are available in mokken: pmatrix, mscpm, and IT
knitr::kable(summary(miio.mysubscale1)$item.summary, caption = Tab_2.5a_cap)
# Investigate the assumption of non-intersecting item step response functions (ISRFs) at different minsize values
miio.59.mysubscale1 <- check.iio(mysubscale1, method="MIIO", minsize = 59)
summary(miio.59.mysubscale1)
miio.50.mysubscale1 <- check.iio(mysubscale1, minsize = 50)
summary(miio.50.mysubscale1)
#miio.30.mysubscale1 <- check.iio(mysubscale1, minsize = 30)
#summary(miio.30.mysubscale1)
par( mfrow=c(6,5));
plot(miio.50.mysubscale1, ask=FALSE)
#__________________#
####   STEP 3   ####
#__________________#
# PIRT ####
# examine PIRT model fit for the unidimensional scales identified via NIRT
# for binary items - Rasch model
#fit1.Subscale1 <- RM(mydata[,Subscale1])
# for ordinal items - Rating Scale model (if all items have the same format, e.g. all are on 5-point scales from strongly agree to strongly disagree)
fit1.Subscale1 <- RSM(mydata[,Subscale1]#, se=TRUE, sum0=TRUE#, constrained = FALSE
)
## # Note: Partial Credit model (if items have different formats, e.g. some are 3-point scales, some 5-point scales)
#fit1.Subscale1 <- PCM(mydata[,Subscale1] #, constrained = FALSE, Hessian=TRUE
# )
# model fit and plotting - same code for binary and ordinal
Tab_2b.1a_cap <- table_nums(name="tab_2b.1a", caption = "R(S)M: Subcale 1: Summary item fit")
Fig_2b.1a_cap <- figure_nums(name="fig_2b.1a", caption = "R(S)M: Subcale 1: Item Characteristic Curves - single plot" )
Fig_2b.2a_cap <- figure_nums(name="fig_2b.2a", caption = "R(S)M: Subcale 1: Item Characteristic Curves - separate plots" )
Fig_2b.3a_cap <- figure_nums(name="fig_2b.3a", caption = "R(S)M: Subcale 1: Person-Item Map" )
Fig_2b.4a_cap <- figure_nums(name="fig_2b.4a", caption = "R(S)M: Subcale 1: Pathway Map" )
Fig_2b.5a_cap <- figure_nums(name="fig_2b.5a", caption = "R(S)M: Subcale 1: Item difficulty for high and low latent score groups" )
Fig_2b.6a_cap <- figure_nums(name="fig_2b.6a", caption = "R(S)M: Subcale 1: Item parameter confidence intervals based on LR test" )
# summary(fit1.Subscale1)
ppr <- person.parameter(fit1.Subscale1)
#ppr <- person.parameter(fit1.Subscale1)
# goodness of fit indices
#gofIRT(ppr)
# information criteria
IC(ppr)
# item-pair residuals for testing local dependencies:
# fit model with lrm package (eRm does not include these tests)
# for binary items: Rasch model
#fit1.ltm.Subscale1 <- rasch(mydata[,Subscale1], constraint = cbind(length(mydata[,Subscale1]) + 1, 1))
# for ordinal items: 1-parameter GRM (the RSM model is not (yet) implemented in ltm)
#fit1.ltm.Subscale1 <- grm(preparedDataSet[,4:14], constrained =TRUE) original
fit1.ltm.Subscale1 <- grm(mydata, constrained =TRUE)
# model summary (item coefficients are item difficulties with standard errors and standardized z values)
summary(fit1.ltm.Subscale1)
# check model fit ( GoF should be ns)
#GoF.rasch(fit1.ltm.Subscale1, B = 199)
# residuals item pairs ( chisq residuals < 3.5 is good - rule of thumb)
margins(fit1.ltm.Subscale1)
# residuals item triplets
margins(fit1.ltm.Subscale1, type = "three-way", nprint = 2) # prints triplets of items with the highest residual values for each response pattern
# plot all ICCs in a single graph
# plot ICCs in a pdf
# pdf( "./ICCs-mydata1.pdf", width=10, height=10, paper="special" );
#plotjointICC(fit1.Subscale1, main="Item Characteristic Curves",ylab="Probability", legend=FALSE)
plotICC(fit1.Subscale1, main="Item Characteristic Curves",ylab="Probability")
