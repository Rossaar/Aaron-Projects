```{r}

# ATTEND Analysis Script: Analysis of the STNI data, for a first pass validation / psychometrics
#
# Input: "ATTEND_allOO_pulled2022-07-18.rds"
#  * Produced by: ATTEND_allOO_2022-07-18.R in HendricksonLab / Data Pull Scripts
#
# Output:
#   XXXXXX
#
# History:
#   2023-07-05 AFR wrote it

###############################
# I. Set up
###############################
```
```{r}
usersetting="Aaron" 
# Aaron, if you run this script and want to add default options for you that are different, just 
# swap this to "Aaron" and update away (I set up the library path portion this way as
# an example, but you may find other pLaces here that you want to swap things up a bit)

if(usersetting=="Rebecca"){
  .libPaths("C:/LocalR/R-4.0.5/library")
  githubpath="C:\\Users\\vhapughendrr\\GitHub\\"
}else if(usersetting=="Aaron"){
  # Put any library path you want to use instead here, and the path to your local github directory
  .libPaths("C:/LocalR/R-4.2.1/Library")
  githubpath="U:\\My Documents\\GitHub\\"
}else{
  warning("You'll need to set a githubpath for much of the code below to work...")
}

# Settings
importdirectories<-list(
  github=githubpath,
  workingdatasaves="R:\\MIRECC\\STUDIES\\Hendrickson_1588176-01881_ATTEND\\Study Data\\Data\\Analyses\\",
  plotsaves="R:\\MIRECC\\STUDIES\\Hendrickson_1588176-01881_ATTEND\\Study Data\\Data\\Analyses\\STNIplots\\"
)
importfilenames<-list(
  workingdata="ATTEND_allOO_pulled2022-10-27.rds"
 #Original Data File used for analysis: 
   # workingdata="ATTEND_allOO_pulled2022-7-18.rds"
 # workingdata="ATTEND BL AFR Test.rds"
)
saveoptions <- list(
  resave=TRUE,
  savetag="STNI"
)


library(data.table)
library(ggplot2)
library(ggpubr)
library(data.table)
library(devtools)
library(EFAtools)
library(nFactors)
library(car)
library(factoextra)
library(lavaan)
library(readr)
library(readxl)
library(openxlsx)
library(htmlTable)
library(rvest) # for stripping html tags
library(flextable)
library(officer)
library(nlme)
library(psych)
library(psychTools)
library(jtools) # for visualizing lm output
library(ggstance) # needed by jtools
library(broom.mixed) # needed by ggstance!
library(mediation)
library(gridExtra)
library(nlme)
library(ltm)
# These you almost certainly won't need
library(maps)
library(ggmap)

# Just source these, because package version never up to date:
source(paste0(importdirectories$github,"HendricksonLab\\HLUtilities\\R\\Utilities.R"))
source(paste0(importdirectories$github,"HendricksonLab\\CoreInfastructure\\R\\GeneralDataFunctions.R"))
source(paste0(importdirectories$github,"HendricksonLab\\CoreInfastructure\\R\\qualtricsFunctions.R"))

# Functions:


# Plot settings that carry through:
reltextsize=1.3
labelsize=9

#################################
# II. Load the prepared data set, initial prep
#################################

dp <- readRDS(paste0(importdirectories$workingdatasaves,importfilenames$workingdata))
# dp stands for data package, which has a couple of parts:
names(dp)

# The dataQL data table is the main data:
dp$data
# You can see the column names:
names(dp$dataQL)

# Other useful one:
dp$dictionaryQL
names(dp$dictionaryQL)
# You can look at the entry for a particular variable (column in dp$dataQL):
dp$dictionaryQL[VarID=="pcl5total"]

# Clean age:
dp$dataQL[age<21,.(age,attendDem_job)]
# the 3 that are "teens" are clearly legit responses, will change age to missing data
dp$dataQL[age<18,age:=NA]

###########################
# III. Basic characterizations
###########################

#############
# Basic demographics, profile of the responses we've gotten:

# how many responses do we have, with what completeness?
nBL <- length(dp$dataQL[VisitID=="BL"]$qaResponseID)
nFu <- length(dp$dataQL[VisitID!="BL"][trelative>0]$qaResponseID)

# Age
mAge <- mean(dp$dataQL$age,na.rm=TRUE)
sdAge <- sd(dp$dataQL$age,na.rm=TRUE)
rangeAge <- range(dp$dataQL$age,na.rm=TRUE)

# Field
nHCW <- sum(dp$dataQL[VisitID=="BL"]$attendDem_HCW)
nFR <- sum(dp$dataQL[VisitID=="BL"]$attendDem_FR)
nOverlap <- length(dp$dataQL[VisitID=="BL"][attendDem_HCW==TRUE][attendDem_FR==TRUE]$qaStartDate) # 6
nPolice <- sum(dp$dataQL[VisitID=="BL"]$attendDem_police)
nFire <- sum(dp$dataQL[VisitID=="BL"]$attendDem_fire)
nParamedic <- sum(dp$dataQL[VisitID=="BL"]$attendDem_emt)
nPhysician <- sum(dp$dataQL[VisitID=="BL"]$attendDem_physician)
nNurse <- sum(dp$dataQL[VisitID=="BL"]$attendDem_nurse)
nArnppa <- sum(dp$dataQL[VisitID=="BL"]$attendDem_arnppa)
fieldVector <- data.table(nHCW,nFR,nPolice,nFire,nParamedic,nPhysician,nNurse,nArnppa)
fieldVector

###########################dp$dataQL[VisitID=="BL"]$attendDem_HCW
# III. Pull out the STNI data to use, along with some demographics
###########################

# 1) What variables do we want to preserve?

# Demographic and basic ID variables:
var2keep=c("PtID","VisitID","age","gender")

# The STNI itself:
dp$dictionaryQL[Base=="STNI"]$VarID
var2keep=c(var2keep,dp$dictionaryQL[Base=="STNI"]$VarID)

# Let's remind ourselves why there's stni10 and stni10a (and same for 11):
dp$dictionaryQL[VarID=="stni10"]
dp$dictionaryQL[VarID=="stni10a"]
  # The answer is in the FieldLabel for stni10a: 
dp$dictionaryQL[VarID=="stni10a"]$FieldLabel
  # "Adjusted version of stni10 so monotonic"
  # Ie, if you look at the order of the answer choices:
dp$dictionaryQL[VarID=="stni10"]$answerChoices
  # ...they were out of order from the way it would make sense to
  # analyze them, ie, "never woken up sweaty" was the last=4 instead of first=0
  # So, stni10a is the same question but with the answer choices
  # rearranged so that never woken up sweaty is coded as 0
  # (stni11 vs 11a is the same)

# Make the actual tight data set:
data=dp$dataQL[,..var2keep] # note the ".." notation, which is very specific but handy; 
  # see this source for its function: https://stackoverflow.com/questions/12391950/select-assign-to-data-table-when-variable-names-are-stored-in-a-character-vect
dictionary=dp$dictionaryQL[VarID%in%var2keep]

# Pull the demographic variables forward now, because otherwise I forget all the time:
temp=data[VisitID=="BL",.(PtID,age,gender)]
data[,age:=NULL]
data[,gender:=NULL]
data=merge(data,temp,by="PtID",all.x=TRUE,all.y=FALSE)

# Drop any visits without a stniTotal, as we only want complete responses for this purpose:
data=data[!is.na(stniTotal)]
  # CONSIDER: do you want to go back and keep folks with NA for the bed partner dependent questions?

# How many responses does this leave?
dim(data) # 950 respodnses
length(unique(data$PtID)) # ...from 571 individual respondents 



```

```{r}
mydata <- na.omit(read.table(file="R:\\MIRECC\\STUDIES\\Hendrickson_1588176-01881_ATTEND\\Study Data\\Aaron's Folder\\stni89BLrandoraw1no10no7.csv", 
                                      header=TRUE, # first row contains variable names 
                                      sep=","))   # comma is separator 
#EXPLORATORY FACTOR ANALYSIS

mycor <- cor(mydata, method = "spearman")

fa.parallel(mydata, fm='ml',fa= 'fa', quant=.95)
PARALLEL(mydata,percent=95)
fa.diagram(fa(mydata, rotate = "oblimin", nfactors = 4))
fa(mydata, rotate = "oblimin", nfactors = 4)

#if factors are correlated (oblique) the factor loadings are regression coefficients and not correlations and as such can be larger than one in magnitude: https://stats.stackexchange.com/questions/266304/in-factor-analysis-or-in-pca-what-does-it-mean-a-factor-loading-greater-than
```

```{r}

#CONFIRMATORY FACTOR ANALYSIS

mydata2 <- na.omit(read.table(file="R:\\MIRECC\\STUDIES\\Hendrickson_1588176-01881_ATTEND\\Study Data\\Aaron's Folder\\stni89BLrandoraw2no10no7.csv", 
                                      header=TRUE, # first row contains variable names 
                                      sep=","))   # comma is separator 
 mycor2 <-cor(mydata2, method="pearson")
 cfa.model<-'
 a =~ stni5+stni6
 b =~ stni1+stni11a+stni2
 c =~ stni9 + stni8
 d =~ stni3 + stni4'
 
 cfa.est<-lavaan::cfa(cfa.model, data=mydata2)
 summary(cfa.est, fit=TRUE, nd=5)
 
 fitmeasures(cfa.est, c('cfi', 'rmsea', 'bic','chisq','pvalue'))
#with smaller sample sizes and lower degrees of freedom, RMSEA may overestimate badness of fit
 #no 10 and no 7 makes the model fit
```

```{r}

```

```{r}
#CLUSTER Analysis

sdf<-scale(mydata)
sdf<-t(sdf)
res.dist <- get_dist(sdf, method = "spearman")
fviz_dist(res.dist, lab_size=8)

res.km <- eclust(sdf, "kmeans", nstart=10)
fviz_gap_stat(res.km$gap_stat)

res.hc <- eclust(sdf, "hclust")
```

















