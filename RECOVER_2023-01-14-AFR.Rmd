# Script to pull data from RECOVER and prepare it for analysis
#
# Input: Qualtrics survey and data exports, as well as updated varmap
#  (See Set Up section assignment of import directories and filenames)
#   --Site to download TMB data: https://studies.testmybrain.org/recover/data_portal.html 
#     * logins are studyname_1stinitiallastname
#   --For Qualtrics data, download as: numeric values, remove line breaks, split multi-value fields into columns
#
# Output: data file saved as
#   "RECOVER_pulled2022-12-14.rds"
#
# Reference Files:
#   ATTEND and PREDICT Data Pull Scripts
#
# History:
#   2022-02-28 RCH wrote it
#   2023-01-03 RCH adding longitudinal components
#
# ISSUES:
# [  ] WHY IS THE DATA DICTIONARY WRONG FOR THE SUBSTANCE USE QUESTIONS?????????????????????
# [  ] At "merge" step below, need to merge data dictionaries, too... write general function to handle?
# [  ] Data dictionary not being created correctly - basenames not picking up when try to add totals in StandardDataCalculation fields...

###############################
# I. Set up
###############################
```{r}
# Library paths, as pulling deidentified data means working on the a VA computer
.libPaths("C:/LocalR/R-4.2.1/Library")

# Install packages if needed:
#setwd("C:/LocalR/")
#install.packages(c("devtools","data.table","ggplot2","reader","readxl","openxlsx","jsonlite","psych","psychTools"))

# Import directories and options
importoptions<-list(
  studies="RECOVER",
  studylettername="C",
  createNew_varmap=FALSE,
  update_varmap=FALSE,
  new_varmap_name="varmap_RECOVER_2023-01-03.rds",
  split_multiplevaluefields=TRUE # This will affect the way the survey is exported AND the way the varmap is created! (Qualtrics specific)
  # DDE settings are not relevant for purely qualtrics data
)
#studydatadirectoryname="R:\\MIRECC\\STUDIES\\Hendrickson_1629444_RECOVER\\Study Data\\Data\\"
#pulldate="2023-01-03"
importdirectories<-list(
  github="U:\\My Documents\\GitHub\\",
  QualtricsData="R:\\MIRECC\\STUDIES\\Hendrickson_1629444_RECOVER\\Study Data\\Data\\Qualtrics\\Data\\2023-01-13\\",
  QualtricsSurveys="R:\\MIRECC\\STUDIES\\Hendrickson_1629444_RECOVER\\Study Data\\Data\\Qualtrics\\Surveys\\2023-01-13\\",
  TMBdata="R:\\MIRECC\\STUDIES\\Hendrickson_1629444_RECOVER\\Study Data\\Data\\TMB\\2023-01-03\\",
  TMBdatadictionary="R:\\MIRECC\\STUDIES\\Hendrickson_1629444_RECOVER\\Study Data\\Data\\TMB\\DataDictionary\\",
#  QualtricsLists="R:\\MIRECC\\STUDIES\\Hendrickson_1629444_RECOVER\\Study Data\\Data\\Qualtrics\\List exports\\",
  workingdatasaves="R:\\MIRECC\\STUDIES\\Hendrickson_1629444_RECOVER\\Study Data\\Data\\Analyses\\2023-01\\"
)
importfilenames<-list(
  QualtricsData="ALL",
  QualtricsSurveys="ALL",
  #QualtricsLists="ATTEND_Baseline_2020_12_29.csv",
  varmap="varmap_RECOVER_2023-01-03.rds"
#  surveymap="SurveyMap_ATTEND_2021-10-29.csv"
)
saveoptions=list(savetag="PUB")
savefilename <- "RECOVER_pulled2023-01-13b.rds"

# Libraries
#library(devtools)
library(data.table)
library(ggplot2)
library(ggpubr) # really only need for analysis part... err...
library(readr)
library(readxl)
library(openxlsx)
library(jsonlite)
library(psych) # need for factorization
library(jtools)
library(psychTools)
library(stringr)
library(lubridate)
# next three for IP coordinate extraction only
library(sp)
library(rworldmap)
library(spam)
library(rworldxtra)

# Just source these, because package version never up to date:
source(paste0(importdirectories$github,"HendricksonLab\\HLUtilities\\R\\Utilities.R"))
source(paste0(importdirectories$github,"HendricksonLab\\CoreInfastructure\\R\\GeneralDataFunctions.R"))
source(paste0(importdirectories$github,"HendricksonLab\\CoreInfastructure\\R\\qualtricsFunctions.R"))
source(paste0(importdirectories$github,"HendricksonLab\\CoreInfastructure\\R\\VarMapFunctions.R"))
source(paste0(importdirectories$github,"HendricksonLab\\CoreInfastructure\\R\\testmybrainFunctions.R"))

# Reminder of how to install packages that are part of our own github respository (need devtools loaded to do this):
#install(pkg=paste0(importdirectories$github,"\\","HendricksonLab\\HLUtilities"))
#install(pkg=paste0(importdirectories$github,"\\","HendricksonLab\\CoreInfastructure"))
# If you have fiddled with CoreInfastructre, you may need to reinstall it, as well - do this by hitting 'Ctrl + Shift + B'

#################################
# II. Load and process the full data set
#################################

# 1. Load the data in and clean / standardize it
# Initialize the data package
dp<-initializeDataPackage(importdirectories,importoptions)
# Load the survey info
dp <- loadQualtricsSurveys(dp,importdirectories,importfilenames)
# Load up the data!
dp <- loadQualtricsData(dp,importdirectories,importfilenames)
  # This standardizes the data dictionary, as well
  # If you get "Error in read.table(...: no lines available in input" it means you have a data file with zero responses... just delete...?

# 2. Apply the varmap to standardize variable names, do common data calculations
#dp <- UpdateVarmap(dp,importdirectories,importoptions,importfilenames)
dp<-standardizeVariableNames(dp,importoptions,importdirectories)
  # Did you get the warning to hand-edit the varmap? Great! Once you've done it, run this code:
  # dp <- UpdateVarmap(dp,importdirectories,importoptions,importfilenames)
  # importoptions$createNew_varmap=FALSE
  # importoptions$update_varmap=FALSE
  # Now you need to re-run the standardizeVariableNames line, above (if you get a connection error, make sure the excel file is closed..)
#  # This converts the variables to our "HL Standard"

# Deal with duplicated columns before we do standard data calc's, because
# we know these are from different visits:
dp$dataQL <- mergedupcol(dp$dataQL)

dp<-standardDataCalculations(dp)
# This does standard calculations, like measure totals

# Add in TMB stuff....
dp <- loadTMBdata(dp,importdirectories,importfilenames)
dp <- processTMBdata(dp,importoptions)

###########################
# III. Bit of processing to merge...
###########################

# Make times into times
dp$dataQL$qaStartDate <- as.Date(dp$dataQL$StartDate)
dp$dataQL$qaEndDate <- as.Date(dp$dataQL$EndDate)

# gender should be factor not numeric:
if(is.numeric(dp$dataQL$gender)){
  map <- c("F","M","NB","O")
  dp$dataQL$gender <- map[dp$dataQL$gender]
}
# Alt version for when NB is too small N:
dp$dataQL[,gendernnb:=gender]
dp$dataQL[(gender=="NB"|gender=="O"),gendernnb:=NA]

# PtID does better if character (so treated as factor in regressions / by ggplot):
dp$dataQL[,PtID:=paste0(importoptions$studylettername,dp$dataQL$ExternalDataReference)]

# Merge...
dp$data <- merge(dp$dataQL,dp$dataTMB,all.x=TRUE,all.y = FALSE,by=c("PtID","VisitID"))
warning("Not doing a good job merging the data dictionaries yet....")

### Extra still tentative fiddling:
dp$data[,asiNarrow:=sum(c(asi1i1,asi1i2+asi1i3,asi1i7,asi1i8,asi2i1,asi2i2,asi2i3,asi2i7,asi2i8,asi3total),na.rm = TRUE),by=1:nrow(dp$data)]
dp$data[,asiN:=sum(c(asi1i1,asi1i2+asi1i3,asi1i4,asi1i5,asi1i6,asi1i7,asi1i8,asi2i1,asi2i2,asi2i3,asi2i4,asi2i5,asi2i6,asi2i7,asi2i8,asi3total),na.rm = TRUE),by=1:nrow(dp$data)]
dp$data[,asiN2:=sum(c(asi1i1,asi1i2+asi1i3,asi1i7,asi1i8,asi2i1,asi2i2,asi2i3,asi2i7,asi2i8),na.rm = TRUE),by=1:nrow(dp$data)]

dp$data[,asi1totalnosleep:=(asi1i1+asi1i2+asi1i3+asi1i4+asi1i5+asi1i6+asi1i7+asi1i8+asi1i9+asi1i10+asi1i11+asi1i12+asi1i15+asi1i16+asi1i17+asi1i18+asi1i19+asi1i22+asi1i23+asi1i27+asi1i28+asi1i29+asi1i30+asi1i31),by=1:nrow(dp$data)]
dp$data[,asi2totalnosleep:=sum(c(asi2i1,asi2i2,asi2i3,asi2i4,asi2i5,asi2i6,asi2i7,asi2i8,asi2i9,asi2i10,asi2i11,asi2i12,asi2i15,asi2i16,asi2i17,asi2i18,asi2i19,asi2i22,asi2i23,asi2i27,asi2i28,asi2i29,asi2i30,asi2i31),na.rm=TRUE),by=1:nrow(dp$data)]
dp$data[,asi3totalnosleep:=sum(c(asi3i1,asi3i2,asi3i3,asi3i4,asi3i5,asi3i6,asi3i7,asi3i8,asi3i9,asi3i10),na.rm=TRUE),by=1:nrow(dp$data)]
dp$data[,asiSxTotalnosleep:=(asi1totalnosleep+asi2totalnosleep+2*asi3totalnosleep),by=1:nrow(dp$data)]
dp$data[,asiN1mostadrenergic:=sum(c(asi1i1,asi1i2+asi1i3,asi1i7,asi1i8,asi2i1,asi2i2,asi2i3,asi2i7,asi2i8,asi3total),na.rm = TRUE),by=1:nrow(dp$data)]
dp$data[,asiN2peripheral:=sum(c(asi1i1,asi1i2+asi1i3,asi1i4,asi1i5,asi1i6,asi1i7,asi1i8,asi1i27,asi1i28,asi1i28,asi2i1,asi2i2,asi2i3,asi2i4,asi2i5,asi2i6,asi2i7,asi2i8,asi2i27,asi2i28,asi2i29,asi3total),na.rm = TRUE),by=1:nrow(dp$data)]
dp$data[,asiN3mostadrenergicnotpostural:=sum(c(asi1i1,asi1i2+asi1i3,asi1i7,asi1i8,asi2i1,asi2i2,asi2i3,asi2i7,asi2i8),na.rm = TRUE),by=1:nrow(dp$data)]
dp$data[,asiN4peripheralnotpostural:=sum(c(asi1i1,asi1i2+asi1i3,asi1i4,asi1i5,asi1i6,asi1i7,asi1i8,asi1i27,asi1i28,asi1i28,asi2i1,asi2i2,asi2i3,asi2i4,asi2i5,asi2i6,asi2i7,asi2i8,asi2i27,asi2i28,asi2i29),na.rm = TRUE),by=1:nrow(dp$data)]

dp$data[,pcl5notE:=(pcl5clusterB+pcl5clusterC+pcl5clusterD)]

##############
# Do some COVID episode processing that want to keep out of the more frequently re-run analysis script:
dctemplate=data.table(PtID=as.character(NA),
                      originalLetter=as.character(NA),
                      EvCovid1=as.logical(NA),EvCovid2=as.logical(NA),EvCovid3=as.logical(NA),EvCovid4=as.logical(NA),EvCovid5=as.logical(NA),EvCovid6=as.logical(NA),EvCovid7=as.logical(NA),EvCovid8=as.logical(NA),EvCovid8_TEXT=as.character(NA),
                      SxCovid1=as.logical(NA),SxCovid4=as.logical(NA),SxCovid5=as.logical(NA),SxCovid6=as.logical(NA),SxCovid7=as.logical(NA),SxCovid8=as.logical(NA),SxCovid9=as.logical(NA),SxCovid10=as.logical(NA),SxCovid11=as.logical(NA),SxCovid12=as.logical(NA),SxCovid13=as.logical(NA),SxCovid14=as.logical(NA),SxCovid14_TEXT=as.character(NA),
                      whenDiscreteLikelyCovid=as.character(NA))
dc=dctemplate[0,]
for(i in 1:nrow(dp$data)){
  for(l in c("A","B","C","D","E","F")){
    newrow=dctemplate
    newrow$PtID=dp$data[i,]$PtID
    newrow$originalLetter=l
    for(q in 1:8){
      evalstring=paste0("newrow$EvCovid",q,"=dp$data[i,]$EvCovid",l,q)
      eval(parse(text=evalstring))
    }
    evalstring=paste0("newrow$EvCovid8_TEXT=dp$data[i,]$EvCovid",l,"8_TEXT")
    eval(parse(text=evalstring))
    for(q in c(1,4:14)){
      evalstring=paste0("newrow$SxCovid",q,"=dp$data[i,]$SxCovid",l,q)
      eval(parse(text=evalstring))
    }
    evalstring=paste0("newrow$SxCovid14_TEXT=dp$data[i,]$SxCovid",l,"14_TEXT")
    eval(parse(text=evalstring))
    evalstring=paste0("newrow$whenDiscreteLikelyCovid=dp$data[i,]$whenDiscreteLikelyCovid",l)
    eval(parse(text=evalstring))
    
    dc=rbind(dc,newrow)
  }
}

dccopyforref=copy(dc)
dc=copy(dccopyforref)

# Turn the 1/NAs to TRUE/FALSE (do before below, so that aren't catching the dates/text entries!)
names2parse=setdiff(names(dc),c("PtID","originalLetter","whenDiscreteLikelyCovid",
                                "EvCovid8_TEXT","SxCovid14_TEXT","covidDate","nSx"))
dc[, (names2parse) := lapply(.SD, as.logical), .SDcols = names2parse]

# Before messing with the NAs, just use the TRUE columns to filter "empty" rows out:
dc=dc[rowSums(dc==TRUE,na.rm = TRUE)>=1]

# Now figure out how to deal with the NAs appropriately:
for(c in names2parse){
  evalstring=paste0("dc[is.na(",c,"),",c,":=FALSE]")
  eval(parse(text=evalstring))
}
# Now same for the text fields...
dc[SxCovid14_TEXT=="",SxCovid14_TEXT:=NA]
dc[EvCovid8_TEXT=="",EvCovid8_TEXT:=NA]

# Parse the dates:
dc[,covidDate:=as.Date(whenDiscreteLikelyCovid,"%m/%d/%Y")]
# For some reason one has dots instead of dashes(!?)
dc[,covidDateAlt:=as.Date(whenDiscreteLikelyCovid,"%m.%d.%Y")]
dc[!is.na(covidDateAlt),covidDate:=covidDateAlt]
dc[,covidDateAlt:=NULL]

# REF for questions:
# EvCovid: 1) rapid+; 2) PCR+; 3) fam/house contact+; 4) other contact+; 5) fam/close contact with Sx; 6) told by clinician; 7) Sx; 8) Other
# SxCovid: 1) fever/chills; 4) fatigue; 5) SOB/diff br; 6) dry cough; 7) taste/smell; 8) HA; 9) aches; 10) sore throat; 11) nasal cong; 12) nausea/appetite; 13) GI Sx; 14) Other

# Now do some by-episode and then by-participant accounting to port back to our main dataset...
dc[,testpos:=(EvCovid1|EvCovid2),by=1:nrow(dc)]
dc[,contactpos:=(EvCovid3|EvCovid4),by=1:nrow(dc)]
dc[,nSx:=sum(c(SxCovid1,SxCovid4,SxCovid5,SxCovid6,SxCovid7,SxCovid8,SxCovid9,SxCovid10,SxCovid11,SxCovid12,SxCovid13,SxCovid14)==TRUE),by=1:nrow(dc)]

dc[,testandSx:=(testpos&EvCovid7)]
dc[,contactposandSx:=(contactpos&EvCovid7)]
dc[,testorcontactandSx:=(testandSx|contactposandSx)]

# (Peaking briefly)
dc[(testandSx|contactposandSx)]

dcbyPt=dc[,.(covidDateLast=max(covidDate),covidDateFirst=min(covidDate),
             testandSx=max(testandSx),contactposandSx=max(contactposandSx),testorcontactandSx=max(testorcontactandSx),
             nSx=max(nSx),nLikelyCovid=length(covidDate)),by="PtID"]

dp$data=merge(dp$data,dcbyPt,by="PtID",all.x = TRUE)
dp$data[,VisitDate:=as.Date(EndDate)]
dp$data[,timesincefirstcovid:=(VisitDate-covidDateFirst)]
dp$data[,timesincelastcovid:=(VisitDate-covidDateLast)]

dp$covidEpisodeData=dc


### 
# Some fixes before we normalize:

# I don't understand what's going on with the entry without an externalreferenceID??? I'm going to cut it for now:
dp$data=dp$data[PtID!="CNA"]

# COVID_categorization is set after the COVID history block is completed, so people can end up miscategorized if they stop partway through this:
miscat=dp$data[(COVID_categorization==FALSE)&(nLikelyCovid>0)]$PtID # do in 2 steps to simplify fixing the normed version
dp$data[PtID%in%miscat,COVID_categorization:=TRUE]

# For now, skip the folks 2/9/22 and earlier (I think half are test runs anyways???)
dp$data$StartDate=as.Date(dp$data$StartDate)
notearlytests=dp$data[StartDate>as.Date("2022-02-09")]$PtID
# bc big deal, who are we losing?
setdiff(dp$data$PtID,notearlytests)
dp$data=dp$data[PtID%in%notearlytests]

# Not sure what's up here...
#> mean(dp$data$nTBIwoLOC,na.rm=TRUE)
#[1] 39374.06
#> max(dp$data$nTBIwoLOC,na.rm=TRUE)
#[1] 1e+07
dp$data[nTBIwoLOC>10000,nTBIwoLOC:=NA]
dp$data$nTBIwoLOC

# Various ways to look at trauma hx:
dp$data[,lecAtotal3cat:=paste0("Reported traumas: ",as.character(lecAtotal)),by=1:nrow(dp$data)]
dp$data[lecAtotal>=2,lecAtotal3cat:="Reported traumas: 2+"]

##############
# Fraud/bot detection: 

# Filter by... IP,date, and captcha???
# 1) IP:
# Function below chokes on NA's, so slightly odd workaround for now: (core taken from: https://stackoverflow.com/questions/14334970/convert-latitude-and-longitude-coordinates-to-country-name-in-r)
pointsWContext=dp$data[!is.na(LocationLongitude),.(PtID,VisitID,VisitDate,LocationLongitude,LocationLatitude,IPAddress)]
points=pointsWContext[,.(LocationLongitude,LocationLatitude)]
# The single argument to this function, points, is a data.frame in which:
#   - column 1 contains the longitude in degrees
#   - column 2 contains the latitude in degrees
coords2country = function(points)
{  
  #countriesSP <- getMap(resolution='low')
  countriesSP <- getMap(resolution='high') #you could use high res map from rworldxtra if you were concerned about detail
  
  # convert our list of points to a SpatialPoints object
  
  # pointsSP = SpatialPoints(points, proj4string=CRS(" +proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs +towgs84=0,0,0"))
  
  #setting CRS directly to that from rworldmap
  pointsSP = SpatialPoints(points, proj4string=CRS(proj4string(countriesSP)))  
  
  
  # use 'over' to get indices of the Polygons object containing each point 
  indices = over(pointsSP, countriesSP)
  
  # return the ADMIN names of each country
  indices$ADMIN  
  #indices$ISO3 # returns the ISO3 code 
  #indices$continent   # returns the continent (6 continent model)
  #indices$REGION   # returns the continent (7 continent model)
}
pointsWContext$countryIP=indices$NAME
#pointsWContext[VisitID=="1",(.N),by="countryIP"]
dp$data=merge(dp$data,pointsWContext[,.(countryIPmap=countryIP,PtID,VisitID)],by=c("PtID","VisitID"))

# BELOW PROVED TOO INACCURATE to use alone:
# Extract and prep the ranages
ipranges=read_excel_allsheets(paste0(importdirectories$github,"HendricksonLab\\CoreInfastructure\\data\\IPfilters.xlsx"))
nC=length(ipranges)
for(iC in 1:nC){
  ranges=as.data.table(ipranges[[iC]])
  names(ranges)="range"
  ranges[,rangestart:=(strsplit(range,"-")[[1]][1]),by=1:nrow(ranges)]
  ranges[,rangestop:=(strsplit(range,"-")[[1]][2]),by=1:nrow(ranges)]
  ipranges[[iC]]=ranges
}
# Cross against the responses:
pullCountryfromIP=function(ip,ipranges){
  if(is.na(ip)){
    countryOut=NA
  }else{
    nC=length(ipranges)
    countryNames=names(ipranges)
    inCountryRanges=vector(mode="logical",length=nC)
    for(iC in 1:nC){
      ranges=ipranges[[iC]]
      ranges[,ipin:=((ip>=rangestart)&(ip<=rangestop))]
      inCountryRanges[iC]=any(ranges$ipin)
    }
    if(sum(inCountryRanges)>1){
      countryOut="Multiple"
    }else if(sum(inCountryRanges)==1){
      countryOut=countryNames[which(inCountryRanges)]
    }else{
      countryOut="Didn't Match"
    }
  }
  return(countryOut)
}
dp$data[,countryIP:=pullCountryfromIP(IPAddress,ipranges),by=1:nrow(dp$data)]
# Merge the two sources:
dp$data[,susIPscore:=2*((countryIP=="Didn't Match")&(countryIPmap!="United States"))]
# Sus dates...
responsedates=data.table(date=as.Date(dp$data$EndDate))
rd=responsedates[,.(freq=.N),by=date]
x <- rd$freq
y <- scale(x,center=TRUE,scale=TRUE)
rd$freqnormed=y
rd[,susdatepenalty:=0]
rd[freqnormed>1,susdatepenalty:=1]
rd[freqnormed>3,susdatepenalty:=2]
dp$data[,date:=as.Date(EndDate)]
dp$data=merge(dp$data,rd[,.(date,susdatepenalty)],by="date")
# recaptcha
dp$data[,passedrecaptchacredit:=0]
dp$data[Q_RecaptchaScore>.5,passedrecaptchacredit:=-1]

# Add the scores and pick which to toss:
dp$data[,susscore:=(susIPscore+susdatepenalty+passedrecaptchacredit)]

dp$data[VisitID==1][susscore>=1,.(PtID,susscore,susIPscore,susdatepenalty,date)]
dp$data[VisitID==1][susIPscore>1,.(PtID,susscore,susIPscore,susdatepenalty,date)]
dp$data[VisitID!=1][susscore>1,.(PtID,susscore,susIPscore,susdatepenalty,date,VisitID,IPAddress)]

tocut=dp$data[VisitID==1][susscore>=1,.(PtID,VisitID)]
dp$data=dp$data[!(PtID%in%tocut$PtID)]


###################################

# Save a version before norming bc end up coming back here so often...
prenormsavefilename=paste0("prenorm_",savefilename)
saveRDS(dp,paste0(importdirectories$workingdatasaves,prenormsavefilename))
#dp=readRDS(paste0(importdirectories$workingdatasaves,prenormsavefilename))


# Create normalized versions of the clusters/factors/covariates that we may need to use multiple places:
tempdt <- copy(dp$data)
classes <- sapply(tempdt,class)
#unique(classes)
var2norm <- names(classes[which(classes%in%c("numeric","logical","integer"))])
var2norm <- setdiff(var2norm,
                    c("VisitID"))
for(iv in 1:length(var2norm)){
  x <- tempdt[,((var2norm[iv])),with=FALSE]
  y <- scale(x,center=TRUE,scale=TRUE)
  tempdt[,(var2norm[iv]):=y]
}
dp$normed <- tempdt

#########################
# Save:

saveRDS(dp,paste0(importdirectories$workingdatasaves,savefilename))
#dp=readRDS(paste0(importdirectories$workingdatasaves,savefilename))

```

```{r}

#ASI1 Validation
asi1Total=(dp$dataQL[VisitID=="1"]$asi1i1+ dp$dataQL[VisitID=="1"]$asi1i2+ dp$dataQL[VisitID=="1"]$asi1i3+ dp$dataQL[VisitID=="1"]$asi1i4+ dp$dataQL[VisitID=="1"]$asi1i5+ dp$dataQL[VisitID=="1"]$asi1i6+ dp$dataQL[VisitID=="1"]$asi1i7+
dp$dataQL[VisitID=="1"]$asi1i8+ dp$dataQL[VisitID=="1"]$asi1i9+ dp$dataQL[VisitID=="1"]$asi1i10+ dp$dataQL[VisitID=="1"]$asi1i11+ dp$dataQL[VisitID=="1"]$asi1i12+ dp$dataQL[VisitID=="1"]$asi1i13+ dp$dataQL[VisitID=="1"]$asi1i14+ 
dp$dataQL[VisitID=="1"]$asi1i15+ dp$dataQL[VisitID=="1"]$asi1i16+ dp$dataQL[VisitID=="1"]$asi1i17+ dp$dataQL[VisitID=="1"]$asi1i18+ dp$dataQL[VisitID=="1"]$asi1i19+ dp$dataQL[VisitID=="1"]$asi1i20+ dp$dataQL[VisitID=="1"]$asi1i21+
dp$dataQL[VisitID=="1"]$asi1i22+ dp$dataQL[VisitID=="1"]$asi1i23+ dp$dataQL[VisitID=="1"]$asi1i24+ dp$dataQL[VisitID=="1"]$asi1i25+ dp$dataQL[VisitID=="1"]$asi1i26+ dp$dataQL[VisitID=="1"]$asi1i27+
dp$dataQL[VisitID=="1"]$asi1i28+ dp$dataQL[VisitID=="1"]$asi1i29+ dp$dataQL[VisitID=="1"]$asi1i30+ dp$dataQL[VisitID=="1"]$asi1i31)

asi1<-data.table(dp$dataQL[VisitID=="1"]$PtID,
dp$dataQL[VisitID=="1"]$asi1i1, dp$dataQL[VisitID=="1"]$asi1i2, dp$dataQL[VisitID=="1"]$asi1i3, dp$dataQL[VisitID=="1"]$asi1i4, dp$dataQL[VisitID=="1"]$asi1i5, dp$dataQL[VisitID=="1"]$asi1i6, dp$dataQL[VisitID=="1"]$asi1i7,
dp$dataQL[VisitID=="1"]$asi1i8, dp$dataQL[VisitID=="1"]$asi1i9, dp$dataQL[VisitID=="1"]$asi1i10, dp$dataQL[VisitID=="1"]$asi1i11, dp$dataQL[VisitID=="1"]$asi1i12, dp$dataQL[VisitID=="1"]$asi1i13, dp$dataQL[VisitID=="1"]$asi1i14, 
dp$dataQL[VisitID=="1"]$asi1i15, dp$dataQL[VisitID=="1"]$asi1i16, dp$dataQL[VisitID=="1"]$asi1i17, dp$dataQL[VisitID=="1"]$asi1i18, dp$dataQL[VisitID=="1"]$asi1i19, dp$dataQL[VisitID=="1"]$asi1i20, dp$dataQL[VisitID=="1"]$asi1i21,
dp$dataQL[VisitID=="1"]$asi1i22, dp$dataQL[VisitID=="1"]$asi1i23, dp$dataQL[VisitID=="1"]$asi1i24, dp$dataQL[VisitID=="1"]$asi1i25, dp$dataQL[VisitID=="1"]$asi1i26, dp$dataQL[VisitID=="1"]$asi1i27,
dp$dataQL[VisitID=="1"]$asi1i28, dp$dataQL[VisitID=="1"]$asi1i29, dp$dataQL[VisitID=="1"]$asi1i30, dp$dataQL[VisitID=="1"]$asi1i31, asi1Total)

colnames(asi1) <- c("PtID", "asi1i1","asi1i2" ,"asi1i3", "asi1i4", "asi1i5", "asi1i6", "asi1i7", "asi1i8", "asi1i9", "asi1i10", "asi1i11", "asi1i12", "asi1i13", "asi1i14",
                   "asi1i15", "asi1i16", "asi1i17", "asi1i18", "asi1i19", "asi1i20", "asi1i21", "asi1i22", "asi1i23", "asi1i24", "asi1i25", "asi1i26",
                   "asi1i27", "asi1i28", "asi1i29", "asi1i30", "asi1i31", "asi1Total")
#ASI1 with PtID and total removed for accurate cronbach
asi1cronbach<-data.table(
dp$dataQL[VisitID=="1"]$asi1i1, dp$dataQL[VisitID=="1"]$asi1i2, dp$dataQL[VisitID=="1"]$asi1i3, dp$dataQL[VisitID=="1"]$asi1i4, dp$dataQL[VisitID=="1"]$asi1i5, dp$dataQL[VisitID=="1"]$asi1i6, dp$dataQL[VisitID=="1"]$asi1i7,
dp$dataQL[VisitID=="1"]$asi1i8, dp$dataQL[VisitID=="1"]$asi1i9, dp$dataQL[VisitID=="1"]$asi1i10, dp$dataQL[VisitID=="1"]$asi1i11, dp$dataQL[VisitID=="1"]$asi1i12, dp$dataQL[VisitID=="1"]$asi1i13, dp$dataQL[VisitID=="1"]$asi1i14, 
dp$dataQL[VisitID=="1"]$asi1i15, dp$dataQL[VisitID=="1"]$asi1i16, dp$dataQL[VisitID=="1"]$asi1i17, dp$dataQL[VisitID=="1"]$asi1i18, dp$dataQL[VisitID=="1"]$asi1i19, dp$dataQL[VisitID=="1"]$asi1i20, dp$dataQL[VisitID=="1"]$asi1i21,
dp$dataQL[VisitID=="1"]$asi1i22, dp$dataQL[VisitID=="1"]$asi1i23, dp$dataQL[VisitID=="1"]$asi1i24, dp$dataQL[VisitID=="1"]$asi1i25, dp$dataQL[VisitID=="1"]$asi1i26, dp$dataQL[VisitID=="1"]$asi1i27,
dp$dataQL[VisitID=="1"]$asi1i28, dp$dataQL[VisitID=="1"]$asi1i29, dp$dataQL[VisitID=="1"]$asi1i30, dp$dataQL[VisitID=="1"]$asi1i31)

colnames(asi1cronbach) <- c( "asi1i1","asi1i2" ,"asi1i3", "asi1i4", "asi1i5", "asi1i6", "asi1i7", "asi1i8", "asi1i9", "asi1i10", "asi1i11", "asi1i12", "asi1i13", "asi1i14",
                   "asi1i15", "asi1i16", "asi1i17", "asi1i18", "asi1i19", "asi1i20", "asi1i21", "asi1i22", "asi1i23", "asi1i24", "asi1i25", "asi1i26",
                   "asi1i27", "asi1i28", "asi1i29", "asi1i30", "asi1i31")

asi1<-na.omit(asi1)
#, dp$dataQL[VisitID=="1"][VisitID=="1"]$PtID
#dp$dataQL[VisitID=="1"]$asi2i1, dp$dataQL[VisitID=="1"]$asi2i2, dp$dataQL[VisitID=="1"]$asi2i3, dp$dataQL[VisitID=="1"]$asi2i4, dp$dataQL[VisitID=="1"]$asi2i5, dp$dataQL[VisitID=="1"]$asi2i6, dp$dataQL[VisitID=="1"]$asi2i7, 
#dp$dataQL[VisitID=="1"]$asi2i8, dp$dataQL[VisitID=="1"]$asi2i9, dp$dataQL[VisitID=="1"]$asi2i10, dp$dataQL[VisitID=="1"]$asi2i11, dp$dataQL[VisitID=="1"]$asi2i12, dp$dataQL[VisitID=="1"]$asi2i13, dp$dataQL[VisitID=="1"]$asi2i14,
#dp$dataQL[VisitID=="1"]$asi2i15, dp$dataQL[VisitID=="1"]$asi2i16, dp$dataQL[VisitID=="1"]$asi2i17, dp$dataQL[VisitID=="1"]$asi2i18, dp$dataQL[VisitID=="1"]$asi2i19, dp$dataQL[VisitID=="1"]$asi2i20, dp$dataQL[VisitID=="1"]$asi2i21,
#dp$dataQL[VisitID=="1"]$asi2i22, dp$dataQL[VisitID=="1"]$asi2i23, dp$dataQL[VisitID=="1"]$asi2i24, dp$dataQL[VisitID=="1"]$asi2i25, dp$dataQL[VisitID=="1"]$asi2i26, dp$dataQL[VisitID=="1"]$asi2i27, dp$dataQL[VisitID=="1"]$asi2i28,
#dp$dataQL[VisitID=="1"]$asi2i29, dp$dataQL[VisitID=="1"]$asi2i30, dp$dataQL[VisitID=="1"]$asi2i31,

#dp$dataQL[VisitID=="1"]$asi3i1, dp$dataQL[VisitID=="1"]$asi3i2, dp$dataQL[VisitID=="1"]$asi3i3, dp$dataQL[VisitID=="1"]$asi3i4, dp$dataQL[VisitID=="1"]$asi3i5, dp$dataQL[VisitID=="1"]$asi3i6, dp$dataQL[VisitID=="1"]$asi3i7,
#dp$dataQL[VisitID=="1"]$asi3i8, dp$dataQL[VisitID=="1"]$asi3i9, dp$dataQL[VisitID=="1"]$asi3i10


#ASI2 Validation
asi1Total=(dp$dataQL[VisitID=="1"]$asi1i1+ dp$dataQL[VisitID=="1"]$asi1i2+ dp$dataQL[VisitID=="1"]$asi1i3+ dp$dataQL[VisitID=="1"]$asi1i4+ dp$dataQL[VisitID=="1"]$asi1i5+ dp$dataQL[VisitID=="1"]$asi1i6+ dp$dataQL[VisitID=="1"]$asi1i7+
dp$dataQL[VisitID=="1"]$asi1i8+ dp$dataQL[VisitID=="1"]$asi1i9+ dp$dataQL[VisitID=="1"]$asi1i10+ dp$dataQL[VisitID=="1"]$asi1i11+ dp$dataQL[VisitID=="1"]$asi1i12+ dp$dataQL[VisitID=="1"]$asi1i13+ dp$dataQL[VisitID=="1"]$asi1i14+ 
dp$dataQL[VisitID=="1"]$asi1i15+ dp$dataQL[VisitID=="1"]$asi1i16+ dp$dataQL[VisitID=="1"]$asi1i17+ dp$dataQL[VisitID=="1"]$asi1i18+ dp$dataQL[VisitID=="1"]$asi1i19+ dp$dataQL[VisitID=="1"]$asi1i20+ dp$dataQL[VisitID=="1"]$asi1i21+
dp$dataQL[VisitID=="1"]$asi1i22+ dp$dataQL[VisitID=="1"]$asi1i23+ dp$dataQL[VisitID=="1"]$asi1i24+ dp$dataQL[VisitID=="1"]$asi1i25+ dp$dataQL[VisitID=="1"]$asi1i26+ dp$dataQL[VisitID=="1"]$asi1i27+
dp$dataQL[VisitID=="1"]$asi1i28+ dp$dataQL[VisitID=="1"]$asi1i29+ dp$dataQL[VisitID=="1"]$asi1i30+ dp$dataQL[VisitID=="1"]$asi1i31)

asi2Total=(dp$dataQL[VisitID=="1"]$asi2i1+ dp$dataQL[VisitID=="1"]$asi2i2+ dp$dataQL[VisitID=="1"]$asi2i3+ dp$dataQL[VisitID=="1"]$asi2i4+ dp$dataQL[VisitID=="1"]$asi2i5+ dp$dataQL[VisitID=="1"]$asi2i6+ dp$dataQL[VisitID=="1"]$asi2i7+ 
dp$dataQL[VisitID=="1"]$asi2i8+ dp$dataQL[VisitID=="1"]$asi2i9+ dp$dataQL[VisitID=="1"]$asi2i10+ dp$dataQL[VisitID=="1"]$asi2i11+ dp$dataQL[VisitID=="1"]$asi2i12+ dp$dataQL[VisitID=="1"]$asi2i13+ dp$dataQL[VisitID=="1"]$asi2i14+
dp$dataQL[VisitID=="1"]$asi2i15+ dp$dataQL[VisitID=="1"]$asi2i16+ dp$dataQL[VisitID=="1"]$asi2i17+ dp$dataQL[VisitID=="1"]$asi2i18+ dp$dataQL[VisitID=="1"]$asi2i19+ dp$dataQL[VisitID=="1"]$asi2i20+ dp$dataQL[VisitID=="1"]$asi2i21+
dp$dataQL[VisitID=="1"]$asi2i22+ dp$dataQL[VisitID=="1"]$asi2i23+ dp$dataQL[VisitID=="1"]$asi2i24+ dp$dataQL[VisitID=="1"]$asi2i25+ dp$dataQL[VisitID=="1"]$asi2i26+ dp$dataQL[VisitID=="1"]$asi2i27+ dp$dataQL[VisitID=="1"]$asi2i28+
dp$dataQL[VisitID=="1"]$asi2i29+ dp$dataQL[VisitID=="1"]$asi2i30+ dp$dataQL[VisitID=="1"]$asi2i31)

asi<-data.table(
dp$dataQL[VisitID=="1"]$PtID, dp$dataQL[VisitID=="1"]$asi1i1, dp$dataQL[VisitID=="1"]$asi1i2, dp$dataQL[VisitID=="1"]$asi1i3, dp$dataQL[VisitID=="1"]$asi1i4, dp$dataQL[VisitID=="1"]$asi1i5, dp$dataQL[VisitID=="1"]$asi1i6, dp$dataQL[VisitID=="1"]$asi1i7,
dp$dataQL[VisitID=="1"]$asi1i8, dp$dataQL[VisitID=="1"]$asi1i9, dp$dataQL[VisitID=="1"]$asi1i10, dp$dataQL[VisitID=="1"]$asi1i11, dp$dataQL[VisitID=="1"]$asi1i12, dp$dataQL[VisitID=="1"]$asi1i13, dp$dataQL[VisitID=="1"]$asi1i14, 
dp$dataQL[VisitID=="1"]$asi1i15, dp$dataQL[VisitID=="1"]$asi1i16, dp$dataQL[VisitID=="1"]$asi1i17, dp$dataQL[VisitID=="1"]$asi1i18, dp$dataQL[VisitID=="1"]$asi1i19, dp$dataQL[VisitID=="1"]$asi1i20, dp$dataQL[VisitID=="1"]$asi1i21,
dp$dataQL[VisitID=="1"]$asi1i22, dp$dataQL[VisitID=="1"]$asi1i23, dp$dataQL[VisitID=="1"]$asi1i24, dp$dataQL[VisitID=="1"]$asi1i25, dp$dataQL[VisitID=="1"]$asi1i26, dp$dataQL[VisitID=="1"]$asi1i27,
dp$dataQL[VisitID=="1"]$asi1i28, dp$dataQL[VisitID=="1"]$asi1i29, dp$dataQL[VisitID=="1"]$asi1i30, dp$dataQL[VisitID=="1"]$asi1i31, asi1Total, 

dp$dataQL[VisitID=="1"]$asi2i1, dp$dataQL[VisitID=="1"]$asi2i2, dp$dataQL[VisitID=="1"]$asi2i3, dp$dataQL[VisitID=="1"]$asi2i4, dp$dataQL[VisitID=="1"]$asi2i5, dp$dataQL[VisitID=="1"]$asi2i6, dp$dataQL[VisitID=="1"]$asi2i7, 
dp$dataQL[VisitID=="1"]$asi2i8, dp$dataQL[VisitID=="1"]$asi2i9, dp$dataQL[VisitID=="1"]$asi2i10, dp$dataQL[VisitID=="1"]$asi2i11, dp$dataQL[VisitID=="1"]$asi2i12, dp$dataQL[VisitID=="1"]$asi2i13, dp$dataQL[VisitID=="1"]$asi2i14,
dp$dataQL[VisitID=="1"]$asi2i15, dp$dataQL[VisitID=="1"]$asi2i16, dp$dataQL[VisitID=="1"]$asi2i17, dp$dataQL[VisitID=="1"]$asi2i18, dp$dataQL[VisitID=="1"]$asi2i19, dp$dataQL[VisitID=="1"]$asi2i20, dp$dataQL[VisitID=="1"]$asi2i21,
dp$dataQL[VisitID=="1"]$asi2i22, dp$dataQL[VisitID=="1"]$asi2i23, dp$dataQL[VisitID=="1"]$asi2i24, dp$dataQL[VisitID=="1"]$asi2i25, dp$dataQL[VisitID=="1"]$asi2i26, dp$dataQL[VisitID=="1"]$asi2i27, dp$dataQL[VisitID=="1"]$asi2i28,
dp$dataQL[VisitID=="1"]$asi2i29, dp$dataQL[VisitID=="1"]$asi2i30, dp$dataQL[VisitID=="1"]$asi2i31, asi2Total)
colnames(asi) <- c("PtID", "asi1i1","asi1i2" ,"asi1i3", "asi1i4", "asi1i5", "asi1i6", "asi1i7", "asi1i8", "asi1i9", "asi1i10", "asi1i11", "asi1i12", "asi1i13", "asi1i14",
                   "asi1i15", "asi1i16", "asi1i17", "asi1i18", "asi1i19", "asi1i20", "asi1i21", "asi1i22", "asi1i23", "asi1i24", "asi1i25", "asi1i26",
                   "asi1i27", "asi1i28", "asi1i29", "asi1i30", "asi1i31", "asi1Total"
                   , "asi2i1",  "asi2i2", "asi2i3", "asi2i4", "asi2i5", "asi2i6",
                   "asi2i7", "asi2i8", "asi2i9", "asi2i10", "asi2i11", "asi2i12", "asi2i13", "asi2i14", "asi2i15", "asi2i16",
                   "asi2i17", "asi2i18", "asi2i19", "asi2i20", "asi2i21", "asi2i22", "asi2i23", "asi2i24", "asi2i25", "asi2i26", "asi2i27",
                   "asi2i28", "asi2i29", "asi2i30", "asi2i31", "asi2Total")#asi3i1", "asi3i2", "asi3i3", "asi3i4", "asi3i5", "asi3i6", "asi3i7",
                   #"asi3i8", "asi3i9", "asi3i10"

#Takes only completed ASI1 responses and then adds ASI2 to them                   
asidupe<-asi
asi<-data.table()
for (i in 1:398){
if (asidupe$PtID[i]%in%asi1$PtID){
    asi<- rbind(asi, asidupe[i])
}
  else{
    #asi<-asi[-i]
  }
}
#asi[is.na(asi)]<- 0

#Making an ASI2 data table where NAs are 0s
#asi2Total=(asi$asi2i1+ asi$asi2i2+ asi$asi2i3+ asi$asi2i4+ asi$asi2i5+ asi$asi2i6+ asi$asi2i7+ 
#asi$asi2i8+ asi$asi2i9+ asi$asi2i10+ asi$asi2i11+ asi$asi2i12+ asi$asi2i13+ asi$asi2i14+
#asi$asi2i15+ asi$asi2i16+ asi$asi2i17+ asi$asi2i18+ asi$asi2i19+ asi$asi2i20+ asi$asi2i21+
#asi$asi2i22+ asi$asi2i23+ asi$asi2i24+ asi$asi2i25+ asi$asi2i26+ asi$asi2i27+ asi$asi2i28+
#asi$asi2i29+ asi$asi2i30+ asi$asi2i31)

asi2=data.table(asi$asi2i1, asi$asi2i2, asi$asi2i3, asi$asi2i4, asi$asi2i5, asi$asi2i6, asi$asi2i7, 
asi$asi2i8, asi$asi2i9, asi$asi2i10, asi$asi2i11, asi$asi2i12, asi$asi2i13, asi$asi2i14,
asi$asi2i15, asi$asi2i16, asi$asi2i17, asi$asi2i18, asi$asi2i19, asi$asi2i20, asi$asi2i21,
asi$asi2i22, asi$asi2i23, asi$asi2i24, asi$asi2i25, asi$asi2i26, asi$asi2i27, asi$asi2i28,
asi$asi2i29, asi$asi2i30, asi$asi2i31)

asi2[is.na(asi2)]<- 0

asi2Total=(asi2$V1+ asi2$V2+ asi2$V3+ asi2$V4+ asi2$V5+ asi2$V6+ asi2$V7+ 
asi2$V8+ asi2$V9+ asi2$V10+ asi2$V11+ asi2$V12+ asi2$V13+ asi2$V14+
asi2$V15+ asi2$V16+ asi2$V17+ asi2$V18+ asi2$V19+ asi2$V20+ asi2$V21+
asi2$V22+ asi2$V23+ asi2$V24+ asi2$V25+ asi2$V26+ asi2$V27+ asi2$V28+
asi2$V29+ asi2$V30+ asi2$V31)

asi2 <- cbind(asi2, asi2Total)
#View(asi2)
library(ltm)
cronbach.alpha(na.omit(asi1cronbach))

summary(lm(asi2$asi2Total~asi$asi1Total))

cor.test(dp$dataQL[VisitID=="1"]$asi1total, dp$dataQL[VisitID=="1"]$pcl5total)
cor.test(dp$dataQL[VisitID=="1"]$asi1total, dp$dataQL[VisitID=="1"]$COMPASS_total)

ggplot(data=dp$dataQL[VisitID=="1"],aes(x=asi1total,y=dp$dataQL[VisitID=="1"]$pcl5total))+
  geom_point(aes(color=COVID_categorization))+stat_cor(method="pearson")+ geom_smooth(method=lm,se=FALSE,color="black",fullrange=TRUE)
```

```{r}
#Making a dataset for easier Linear Regression Modelling
ds <- data.table(dp$dataQL[VisitID=="1"]$PtID , dp$dataQL[VisitID=="1"]$gender, dp$dataQL[VisitID=="1"]$pLikelyCovid, dp$dataQL[VisitID=="1"]$pcl5total,
                 
dp$dataQL[VisitID=="1"]$asi1i1, dp$dataQL[VisitID=="1"]$asi1i2, dp$dataQL[VisitID=="1"]$asi1i3, dp$dataQL[VisitID=="1"]$asi1i4, dp$dataQL[VisitID=="1"]$asi1i5, dp$dataQL[VisitID=="1"]$asi1i6, dp$dataQL[VisitID=="1"]$asi1i7,
dp$dataQL[VisitID=="1"]$asi1i8, dp$dataQL[VisitID=="1"]$asi1i9, dp$dataQL[VisitID=="1"]$asi1i10, dp$dataQL[VisitID=="1"]$asi1i11, dp$dataQL[VisitID=="1"]$asi1i12, dp$dataQL[VisitID=="1"]$asi1i13, dp$dataQL[VisitID=="1"]$asi1i14, 
dp$dataQL[VisitID=="1"]$asi1i15, dp$dataQL[VisitID=="1"]$asi1i16, dp$dataQL[VisitID=="1"]$asi1i17, dp$dataQL[VisitID=="1"]$asi1i18, dp$dataQL[VisitID=="1"]$asi1i19, dp$dataQL[VisitID=="1"]$asi1i20, dp$dataQL[VisitID=="1"]$asi1i21,
dp$dataQL[VisitID=="1"]$asi1i22, dp$dataQL[VisitID=="1"]$asi1i23, dp$dataQL[VisitID=="1"]$asi1i24, dp$dataQL[VisitID=="1"]$asi1i25, dp$dataQL[VisitID=="1"]$asi1i26, dp$dataQL[VisitID=="1"]$asi1i27,
dp$dataQL[VisitID=="1"]$asi1i28, dp$dataQL[VisitID=="1"]$asi1i29, dp$dataQL[VisitID=="1"]$asi1i30, dp$dataQL[VisitID=="1"]$asi1i31, asi1Total, 

dp$dataQL[VisitID=="1"]$asi2i1, dp$dataQL[VisitID=="1"]$asi2i2, dp$dataQL[VisitID=="1"]$asi2i3, dp$dataQL[VisitID=="1"]$asi2i4, dp$dataQL[VisitID=="1"]$asi2i5, dp$dataQL[VisitID=="1"]$asi2i6, dp$dataQL[VisitID=="1"]$asi2i7, 
dp$dataQL[VisitID=="1"]$asi2i8, dp$dataQL[VisitID=="1"]$asi2i9, dp$dataQL[VisitID=="1"]$asi2i10, dp$dataQL[VisitID=="1"]$asi2i11, dp$dataQL[VisitID=="1"]$asi2i12, dp$dataQL[VisitID=="1"]$asi2i13, dp$dataQL[VisitID=="1"]$asi2i14,
dp$dataQL[VisitID=="1"]$asi2i15, dp$dataQL[VisitID=="1"]$asi2i16, dp$dataQL[VisitID=="1"]$asi2i17, dp$dataQL[VisitID=="1"]$asi2i18, dp$dataQL[VisitID=="1"]$asi2i19, dp$dataQL[VisitID=="1"]$asi2i20, dp$dataQL[VisitID=="1"]$asi2i21,
dp$dataQL[VisitID=="1"]$asi2i22, dp$dataQL[VisitID=="1"]$asi2i23, dp$dataQL[VisitID=="1"]$asi2i24, dp$dataQL[VisitID=="1"]$asi2i25, dp$dataQL[VisitID=="1"]$asi2i26, dp$dataQL[VisitID=="1"]$asi2i27, dp$dataQL[VisitID=="1"]$asi2i28,
dp$dataQL[VisitID=="1"]$asi2i29, dp$dataQL[VisitID=="1"]$asi2i30, dp$dataQL[VisitID=="1"]$asi2i31,

dp$dataQL[VisitID=="1"]$asi3i1, dp$dataQL[VisitID=="1"]$asi3i2, dp$dataQL[VisitID=="1"]$asi3i3, dp$dataQL[VisitID=="1"]$asi3i4, dp$dataQL[VisitID=="1"]$asi3i5, dp$dataQL[VisitID=="1"]$asi3i6, dp$dataQL[VisitID=="1"]$asi3i7, dp$dataQL[VisitID=="1"]$asi3i8, dp$dataQL[VisitID=="1"]$asi3i9, dp$dataQL[VisitID=="1"]$asi3i10)

colnames(ds) <- c("PtID", "Gender", "C19", "pcl5Total", "asi1i1","asi1i2" ,"asi1i3", "asi1i4", "asi1i5", "asi1i6", "asi1i7", "asi1i8", "asi1i9", "asi1i10", "asi1i11", "asi1i12", "asi1i13", "asi1i14",
                   "asi1i15", "asi1i16", "asi1i17", "asi1i18", "asi1i19", "asi1i20", "asi1i21", "asi1i22", "asi1i23", "asi1i24", "asi1i25", "asi1i26",
                   "asi1i27", "asi1i28", "asi1i29", "asi1i30", "asi1i31", "asi1Total"
                   , "asi2i1",  "asi2i2", "asi2i3", "asi2i4", "asi2i5", "asi2i6",
                   "asi2i7", "asi2i8", "asi2i9", "asi2i10", "asi2i11", "asi2i12", "asi2i13", "asi2i14", "asi2i15", "asi2i16",
                   "asi2i17", "asi2i18", "asi2i19", "asi2i20", "asi2i21", "asi2i22", "asi2i23", "asi2i24", "asi2i25", "asi2i26", "asi2i27",
                   "asi2i28", "asi2i29", "asi2i30", "asi2i31", "asi3i1", "asi3i2", "asi3i3", "asi3i4", "asi3i5", "asi3i6", "asi3i7",
                   "asi3i8", "asi3i9", "asi3i10")

summary(lm(ds$asi1Total~ds$pcl5Total, data=ds))
corr.test(ds$asi1Total, ds$pcl5Total)
summary(lm(ds$pcl5Total~asi1$asi1Total+asi2$asi2Total))
```









